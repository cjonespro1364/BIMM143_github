---
title: "Class 7: Machine Learning I"
author: Cameron Jones
format: pdf
---

In this class we will explore clustering and dimensionality reduction methods. 

## K-means

Make up some input data where we know what the answer should be. 

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, +3))
x <- cbind(x=tmp, y=rev(tmp))
head(x)
```
Quick plot of x to see the two groups at -3,+3 and +3,-3
```{r}
plot(x)
```

Use the 'kmeans()' function setting k to 2 and nstart=20

```{r}
km <- kmeans(x, centers = 2, nstart=20)
km
```



>Q. How many points are in each cluster?

```{r}
km$size
```


>Q. What 'component' of your results object details
  -cluster assignment/membership?
  -cluster center?

```{r}
km$cluster
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points


```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

Play with kmeans and ask for different number of clusters
```{r}
km <- kmeans(x, centers = 4, nstart=20)
plot(x, col=km$cluster)
points(km$clusters, col="blue", pch=16, cex=2)
```

#Hierarchical Clustering

This is another very useful and widely employed clustering method which has the advantage over k-means in that it an help reveal the something of the true grouping in your data.

The 'hclust()' function wants a distance matrix as input. We can get this from the 'dist()' function.
```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

There is a plot method for hclust results:
```{r}
plot(hc)
abline(h=10, col="red")
```

To get my cluster membership vector, I need to "cut" my tree to yield sub-trees or branches with all the members of a given cluster residing on the same cut branch. The function to do this is called 'cutree()'

```{r}
grps <- cutree(hc, h=10)
grps
```
```{r}
plot(x, col=grps)
```

It is often helpful to use the 'k=' argument to cutree rather than the height 'h=' of cutting with 'cutree()'. This will cut the tree to yield the number of clusters you want.
```{r}
cutree(hc, k=2)
```

# Principal Component Analysis (PCA)

The base R function for PCA is called 'prcomp()'
Let's play with some 17D data

##PCA of UK food Data

Import the data

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
head(x)
```
>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
nrow(x)
ncol(x)
```

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```
```{r}
x <- read.csv(url, row.names=1)
head(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second approach  is better, as the first approach, if repeated, will start deleting columns!

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
```{r}
pairs(x, col=rainbow(10), pch=16)
```

```{r}
pca <- prcomp(t(x))
summary(pca)
```

A "PCA" plot (aka "Score plot", PC1vsPC2 plot, etc.)

```{r}
pca$x
```
```{r}
plot(pca$x[,1], pca$x[,2], 
     col=c("orange", "red", "blue", "darkgreen"), pch=15)
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
```{r}
z <- summary(pca)
z$importance
```
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```


